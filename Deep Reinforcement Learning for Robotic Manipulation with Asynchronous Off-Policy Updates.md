# Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates

[TOC]

# Abstract

强化学习有希望使全自主机器人在最少的人工干预下学习大型复杂的行为。但是，为例实现对于真是物理系统而言实用的训练时间，强化学习在机器人上的应用通常会损害学习过程的自主性。这通常涉及引入手工设计的策略表达与人工提供的示范。深度强化学习通过训练具有泛化性的通用神经网络策略来缓解这种限制，但深度强化学习算法的直接应用迄今仅限于模拟设置和相对简单的任务，因为它们明显具有高样本复杂性。在本文中，我们证明了最近基于deep Q-functions的off-policy策略训练的深度强化学习算法可以扩展到复杂的3D操作任务，并且可以有效地学习深度神经网络策略以训练真实的物理机器人。我们的实验评估表明，我们的方法可以在模拟中学习各种3D操作技能，并且可以在没有任何先前演示或手动设计的情况下在真实机器人上学习复杂的开门技巧。

# Introduction

强化学习方法已经广泛应用于机器人控制任务，从简单运动到复杂操作和全自主移动机器人控制。然而，强化学习的实际应用通常需要超出学习算法本身的大量额外工程：必须选择适当的策略或者价值函数表示，以便实现对物理硬件实用的训练时间，并且通常必须提供实例演示来初始化策略并减轻训练期间的安全问题。

在本文中，我们表明最近提出的基于deep Q-functions的off-policy的深度强化学习算法可以扩展到从零开始学习复杂的操作策略，无需提供演示示例。仅使用可泛化的通用神经网络而不需要任务特定领域的知识。

将深度强化学习算法直接应用于现实世界的机器人平台的主要挑战之一是它们明显的高复杂性。我们证明，与通常的假设相反，最近开发的基于deep Q-functions的off-policy的算法，例如DDPG和NAF，可以实现适用于真实机器人系统的训练时间。我们还证明，通过多个机器人平台上并行化算法，我们可以进一步缩短训练时间。为此，我们提出了一种新的NAF异步变体，评估在模拟中使用不同数量的学习者获得的加速，并通过多个机器人的并行展示真实世界的结果。

本文的主要贡献是在一组机器人中使用我们的并行NAF算法演示异步深度强化学习。我们的技术贡献包括NAF算法的异步变体，以及该方法的实际扩展，以在真是机器人平台上实现样本有效的训练。我们还介绍了一种简单有效的安全机制，用于在训练时限制探索，并提供模拟实验，评估从不同数量的学习者并行化获得的加速。我们的实验还通过与更标准的线性表示进行比较，评估了深度神经网络表示对几个复杂操作人物的好处，包括开门和拾取放置物体。我们的真是世界实验表明，我们的方法可以用与从零开始学习开门技能，仅仅使用通用神经网络而无需任何人体演示。

# Related Work

强化学习在机器人学中的应用已经包括运动、操纵和自主移动机器人控制。由于难以有效地优化高位策略参数向量，在物理机器人上使用的许多RL方法都是用了相对低维的策略表示，通常具有不到一百个参数。尽管已经对通用神经网络的强化学习进行了一段时间的大量研究，但这种方法最近才发展到可以应用于高维系统的连续控制的程度，例如 7 degree-of-freedom (DoF)手臂。这使得用最少的人工设计学习复杂技能成为可能，尽管由于样本的复杂性，这些方法是否可以适应实际系统仍然不清楚。

在机器人学习场景中，先前的工作已经探索了基于模型和无模型的学习算法。基于模型的算法已经探索了各种动态估计方案，包括高斯过程(Gaussian Process)，混合模型(Mixture Model)和局部线性系统估计(Local Linear System Estimation)。在启发性策略搜索算法的背景下，深度神经网络策略已与基于模型的学习相结合，该算法使用基于模型的教师在训练深层网络策略。这些方法在一系列实际任务中取得了成功，但依赖于基于模型的教师发现到目标的良好轨迹的能力。如最近的工作所示，这在动态和reward function具有严重不连续性的领域中可能是困难的。

在这项工作中，我们专注于model-free强化学习，其中包括策略搜索方法和函数逼近方法。最近，这两种方法都与深度神经网络相结合，用于学习复杂任务。直接策略梯度方法提供了无偏梯度估计的好处，但往往需要更多的经验，因为on-policy的评估器排除了过去数据的重用。另一方面，基于Q-function估计的方法通常允许我们利用过去的off-policy数据。因此，我们建立了基于Q-function的方法，扩展了NAF，将Q-learning扩展到连续空间。这种方法与DDPG以及NFQCA密切相关，主要区别在于NFQCA采用批量情节更新并且通常会重置episode之间的网络参数。

通过汇集来自多个机器人的经验来加速机器人学习一直被认为是机器人领域的一个具有前景的方向，其通常被称为collective robotic learning。在深度强化学习中，还提出了并行学习来加速模拟实验。这项先前工作的目标与我们的基本不同：虽然先前的异步深度强化学习工作旨在减少整体训练时间，但前提是假设模拟时间不昂贵且训练由神经网络计算主导。我们的工作反而力求最小化训练真是物理机器人的训练时间，训练经验代价昂贵且计算神经网络后向传导相对便宜。

在这种情况下，我们保留使用重放缓冲区，并专注于异步执行和神经网络训练。我们的结果表明，通过在多个机器人平台上同时收集经验，在整体训练时间内实现了显著的加速。这个结果与并行应到策略探索的并发结果一致，实际上我们处理类似的开门任务。与此并发工作相比，我们从头开始执行任务，而无需从用户演示初始化算法。虽然并行工作提供了对物理变化推广的更广泛探索，但我们专注于分析model-free深度强化学习方法的学习速度。

# BackGround

在本章我们将公式化机器人强化学习问题，给出必要的公式，描述现存的算法基础和我们的方法。强化学习的目的是控制一个智能体agent尝试最大化reward function，reward function表示用户提供的机器人应该完成的任务的定义。在状态$s_t$在时间 $t$，智能体选择和执行动作$a_t$根据策略$\pi(a_t|s_t)$，转移到一个新的状态$s_t$根据状态转移概率$p(s_t|s_t,a_t)$同时收到一个奖励$r(s_t,a_t)$。这里，我们考虑infinite-horizon discounted回报问题，即目标是以$\gamma$-discounted的未来返回，从时间$t$到$\infin$，即$R_t = \sum_{i=t}^{\infin} \gamma^{(i-t)}r(s_i, a_i)$。目标为找到一个最优的策略$\pi^*$使得从初始状态分布得到的期望回报最大$R=\Bbb{E}_{\pi}[R_0]$。

在强化学习方法中，例如Q-learning的off-policy方法相较于on-policy方有显著的数据利用效率，这对于机器人应用极为关键。 Q-learning训练一个贪心的确定性策略$\pi(a_t|x_t)=\delta(a_t=\mu(s_t))$ 通过在Q-function中迭代学习。考虑连续动作空间上的问题，策略更新对于一个由神经网络参数定义的Q-function是一个棘手的问题。因此，我们研究了DDPG和NAF。DDPG通过采用actor-critic方法来规避问题，而NAF将Q-function限制为下面的表达式以启用closed-form更新，如在离散动作情况中的那样。在探索期间，将事件相关的噪声添加到策略网络的输出中。

$$Q(s,a|\theta^Q)=A(s,a|\theta^A) + V(s|\theta^V)$$

$$A(s,a|\theta^A)=-\frac{1}{2}(a-\mu(a|\theta^{\mu}))^TP(s|\theta^P)(a-\mu(x|\theta^\mu))$$$

