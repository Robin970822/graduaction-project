%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   LaTeX File for Doctor (Master) Thesis of Tsinghua University
%   LaTeX + CJK     清华大学博士\KH{硕士}论文模板
%   Based on Wang Tianshu's Template for XJTU
%   Version: 1.00
%   Last Update: 2003-09-12
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Copyright 2002-2003  by  Lei Wang (BaconChina)       (bcpub@sina.com)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   LaTeX File for phd thesis of xi'an Jiao Tong University
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Copyright 2002  by  Wang Tianshu    (tswang@asia.com)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Latex 西安交通大学博士论文的模板.
%
% 建议使用miktex2.1最大安装编译此模板
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%draft 选项可以使插入的图形只显示外框，以加快预览速度。
%fleqn 让公式左对齐。
\documentclass[12pt,a4paper,openany,oneside]{book}
%\documentclass[11pt,a4paper,openany,draft]{book}
%\documentclass[11pt,a4paper,fleqn,openany,draft]{book}
%\documentclass[11pt,a4paper,fleqn,openany,draft]{book}

%以下是采用dvipdfmx所需设置
%\AtBeginDvi{\special{pdf:tounicode GBK-EUC-UCS2}}
%\usepackage[CJKbookmarks=true,dvipdfm,
%           hyperindex=true,
%           pdfstartview=FitH,
%           bookmarksnumbered=true,
%           bookmarksopen=true,
%           colorlinks=true, %注释掉此项则交叉引用为彩色边框(将colorlinks和pdfborder同时注释掉)
%           pdfborder=001,   %注释掉此项则交叉引用为彩色边框
%           citecolor=blue%
%           ]{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 引用的宏包
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{setup/package.tex}

\begin{document}

%定义所有的eps文件在 figures 子目录下
\graphicspath{{figures/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  文本格式定义
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{setup/format.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% 正文部分
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--- Preface ------------------------
\frontmatter

% 解决中英文混排的断行问题，会加入间距，但不会影响断行
\sloppy

\pagenumbering{Roman}

%封面
\include{preface/cover}

%授权
%\include{preface/authorization}

\setcounter{page}{1}

%中文摘要
\include{preface/c_abstract}

%英文摘要
\include{preface/e_abstract}
%目录
\renewcommand{\baselinestretch}{1.25}
\fontsize{12pt}{12pt}\selectfont

\tableofcontents

%符号对照表
%\include{preface/denotation}

\mainmatter

\renewcommand{\baselinestretch}{1.5}

% 对应于小四的标准字号是 12pt
% 可以在正文中用此命令修改所需要字体的的大小
%\fontsize{12pt}{13pt}\selectfont
\xiaosi\song


%--- body --------------------------

%正文章节
\include{body/preface}
\include{body/introduction}
\include{body/rl}
\include{body/experiment}

% 结论
\include{body/conclusion}

%参考文献
\wuhao

%\bibliographystyle{unsrt}
\bibliographystyle{GBT7714-2005NLang}
%\ifpdf \phantomsection \fi

%\addcontentsline{toc}{chapter}{\hei 参考文献}
%\addtolength{\itemsep}{-0.8 em} % 缩小参考文献间的垂直间距, 在bibtex下无效
%\bibliography{reference/reference}

% 致谢

\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{{\bf 参考文献}}
%\addtolength{\itemsep}{-0.5pt}  %修改参考文献间距

{\renewcommand\baselinestretch{1.25}\selectfont


\begin{thebibliography}{999}
\addtolength{\itemsep}{-1.05 em} % 缩小参考文献间的垂直间距
\setlength{\itemsep}{1pt}
%\renewcommand{\baselinestretch}{1.05}
\bibitem{1}
徐扬生. 智能机器人引领高新技术发展[J]. 企业科协, 2010 (9): 28-31.
\bibitem{2}
Christensen H I, Batzinger T, Bekris K, et al. A roadmap for us robotics: from internet to robotics[J]. Computing Community Consortium, 2009, 44.
\bibitem{3}
Salichs M A, Moreno L. Navigation of mobile robots: open questions[J]. Robotica, 2000, 18(3): 227-234.
\bibitem{4}
Beom H R, Cho H S. A sensor-based navigation for a mobile robot using fuzzy logic and reinforcement learning[J]. IEEE transactions on Systems, Man, and Cybernetics, 1995, 25(3): 464-477.
\bibitem{5}
Pandey A, Sonkar R K, Pandey K K, et al. Path planning navigation of mobile robot with obstacles avoidance using fuzzy logic controller[C]//2014 IEEE 8th International Conference on Intelligent Systems and Control (ISCO). IEEE, 2014: 39-41.
\bibitem{6}
Wang C, Soh Y C, Wang H, et al. A hierarchical genetic algorithm for path planning in a static environment with obstacles[C]//IEEE CCECE2002. Canadian Conference on Electrical and Computer Engineering. Conference Proceedings (Cat. No. 02CH37373). IEEE, 2002, 3: 1652-1657.
\bibitem{7}
Pandey A, Sonkar R K, Pandey K K, et al. Path planning navigation of mobile robot with obstacles avoidance using fuzzy logic controller[C]//2014 IEEE 8th International Conference on Intelligent Systems and Control (ISCO). IEEE, 2014: 39-41.
\bibitem{8}
Sutton R S, Barto A G. Introduction to reinforcement learning[M]. Cambridge: MIT press, 1998.
\bibitem{9}
LeCun Y. Yoshua Bengio, and Geoffrey Hinton[J]. Deep learning. nature, 2015, 521(7553): 436-444.
\bibitem{10}
Bengio Y, Courville A, Vincent P. Representation learning: A review and new perspectives[J]. IEEE transactions on pattern analysis and machine intelligence, 2013, 35(8): 1798-1828.
\bibitem{11}
Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning[J]. Nature, 2015, 518(7540): 529.
\bibitem{12}
Silver D, Huang A, Maddison C J, et al. Mastering the game of Go with deep neural networks and tree search[J]. nature, 2016, 529(7587): 484.
\bibitem{13}
Levine S, Finn C, Darrell T, et al. End-to-end training of deep visuomotor policies[J]. The Journal of Machine Learning Research, 2016, 17(1): 1334-1373.
\bibitem{14}
Levine S, Pastor P, Krizhevsky A, et al. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection[J]. The International Journal of Robotics Research, 2018, 37(4-5): 421-436.
\bibitem{15}
Duan Y, Schulman J, Chen X, et al. RL $^ 2$: Fast Reinforcement Learning via Slow Reinforcement Learning[J]. arXiv preprint arXiv:1611.02779, 2016.
\bibitem{16}
Wang J X, Kurth-Nelson Z, Kumaran D, et al. Prefrontal cortex as a meta-reinforcement learning system[J]. Nature neuroscience, 2018, 21(6): 860.
\bibitem{17}
Vinyals O, Ewalds T, Bartunov S, et al. Starcraft ii: A new challenge for reinforcement learning[J]. arXiv preprint arXiv:1708.04782, 2017.
\bibitem{18}
Kaelbling L P, Littman M L, Cassandra A R. Planning and acting in partially observable stochastic domains[J]. Artificial intelligence, 1998, 101(1-2): 99-134.
\bibitem{19}
Watkins C J C H, Dayan P. Q-learning[J]. Machine learning, 1992, 8(3-4): 279-292.
\bibitem{20}
Rummery G A, Niranjan M. On-line Q-learning using connectionist systems[M]. Cambridge, England: University of Cambridge, Department of Engineering, 1994.
\bibitem{21}
Koutník J, Cuccu G, Schmidhuber J, et al. Evolving large-scale neural networks for vision-based reinforcement learning[C]//Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013: 1061-1068.
\bibitem{22}
Deisenroth M P, Neumann G, Peters J. A survey on policy search for robotics[J]. Foundations and Trends® in Robotics, 2013, 2(1–2): 1-142.
\bibitem{23}
Williams R J. Simple statistical gradient-following algorithms for connectionist reinforcement learning[J]. Machine learning, 1992, 8(3-4): 229-256.
\bibitem{24}
Konda V R, Tsitsiklis J N. Onactor-critic algorithms[J]. SIAM journal on Control and Optimization, 2003, 42(4): 1143-1166.
\bibitem{25}
Schulman J, Moritz P, Levine S, et al. High-dimensional continuous control using generalized advantage estimation[J]. arXiv preprint arXiv:1506.02438, 2015.
\bibitem{26}
Lillicrap T P, Hunt J J, Pritzel A, et al. Continuous control with deep reinforcement learning[J]. arXiv preprint arXiv:1509.02971, 2015.
\bibitem{27}
Gu S, Holly E, Lillicrap T, et al. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates[C]//2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017: 3389-3396.
\bibitem{28}
Van Hasselt H, Guez A, Silver D. Deep reinforcement learning with double q-learning[C]//Thirtieth AAAI Conference on Artificial Intelligence. 2016.
\bibitem{29}
Wang Z, Schaul T, Hessel M, et al. Dueling network architectures for deep reinforcement learning[J]. arXiv preprint arXiv:1511.06581, 2015.
\bibitem{30}
Schaul T, Quan J, Antonoglou I, et al. Prioritized experience replay[J]. arXiv preprint arXiv:1511.05952, 2015.
\end{thebibliography}

\include{appendix/acknowledgements}
\include{appendix/design-conclusion}

%  附录

%\begin{appendix}
%    \renewcommand{\chaptername}{附录\Alph{chapter}}
%   \input{appendix/appendix.tex}
%\end{appendix}

% 发表的文章列表

%\include{appendix/publications}

\clearpage
\end{document}

%%%%%%%%%%%%%%%%%% End of the file  %%%%%%%%%%%%%%%%%%%%%%%%
