\chapter{绪论}\label{preface}

\section{研究背景}
机器人技术是机械、电子、控制、计算机、人工智能等多学科交叉的领域。进入21世纪以来，国内外对机器人技术的发展越来越重视，机器人技术被认为是对未来新兴产业发展具有重要意义的高新技术之一\cite{1}。机器人的研发、制造与应用是衡量一个国家科技创新和高端制造业水平的重要标志。

机器人技术的研究和应用已经从传统的工业领域快速扩展到其他领域，如医疗健康、家政服务、外形探索、勘测勘探等。无论是传统的工业领域还是其他领域，对机器人性能要求的不断提高，使机器人必须面对更极端的环境、完成更复杂的任务。

许多国家加大对机器人技术的研究投入，并将其作为未来新兴产业寄予厚望，是未来高技术、新兴产业竞争的制高点，对于国家经济发展和国防建设具有重要意义。

近年来，我国在国家自然科学基金、863计划以及国家科技重大专项等规划中对机器人技术给予了极高的关注度。国际上，美国启动了“美国国家机器人计划”\cite{2}。欧盟在第七框架计划(FP7) 中规划了“认知系统与机器人技术”研究。日本制定了机器人技术长期发展战略。韩国制定了“智能机器人基本计划”。

\subsection{轮式机器人决策系统研究现状}
轮式移动机器人主要有智能轮椅、导游机器人、野外侦察机器人、大型智能车辆等。其定位、运动规划、自主控制、服务作业等技术和方法也得到广泛研究。随着人工智能、计算机网络技术、传感器技术等新技术的飞速发展，以及工业程度的不断提高，轮式机器人能够更好的服务社会。

目前对于轮式机器人的研究工作，主要集中在路径规划方法上。路径规划是轮式机器人研究领域的关键技术之一，旨在规划一条从起点到目标点的无碰撞路径，同时优化性能指标如距离、时间或者能耗，其中距离是最常采用的方法\cite{3}。

本文着重于在可靠路径规划算法的基础上，即在一台拥有可靠定位、避障和路径规划的机器人上，思考和探索一种能够适应复杂环境做出自主决策的轮式机器人决策系统。目前，模糊逻辑\cite{4}、决策树\cite{5}、状态机、遗传算法\cite{6}、神经网络\cite{7}等都是较为成功有效的轮式机器人决策方法。但这些方法通常需要假设完整的环境信息，然而，在大量的实际应用中需要智能体具有适应不确定环境的能力。因此，如何提高机器人路径规划的自学能力和自适应性成为当前研究的关键技术。

强化学习(Reinforcement Learning, RL)方法通过智能体与位置环境交互，并尝试动作选择使累积回报最大，该方法通常运用马尔可夫决策过程(Markov Decision Processes, MDP)进行环境建模。马尔可夫决策过程模型主要针对理想情况下的单智能体系统，智能体环境的不确定性也可由部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP)进行描述。强化学习算法不需要给定任何状态下的指导信号，只通过智能体与环境交互进行学习并优化控制参数，在先验信息较少的复杂优化决策问题中具有广阔的应用前景。

\subsection{强化学习研究现状}
人工智能的一个首要目标就是生成能够与环境交互，通过尝试与错误学习以优化自身行为的全自主的智能体。创造一个能够有效学习的人工智能系统一直以来是一个长期的挑战，从能够感知和与周边环境进行交互的机器人到与自然语言与多媒体交互的基于软件的智能体。强化学习是一种经验驱动的全自主学习的数学方法框架\cite{8}。

强化学习的目标是需要学习一种策略，即当智能体agent处于一种状态state，做出一个动作action的决策。如果我们将动作看作对状态的标签，强化学习就可以类比监督学习，这样策略就相当于一个分类器或者回归器。主要的区别在于强化学习的数据往往需要通过尝试、和环境进行交互获得。算法则根据环境给予的反馈来调整策略。

强化学习的任务通常使用马尔可夫决策过程描述。智能体agent处于一个环境中，每个状态state为agent对环境的感知。当智能体agent执行一个动作后，环境会按照概率转移到另一个状态；同时，环境会根据奖励函数给予智能体agent一个反馈，通常是奖励reward。综合而言，强化学习主要包含四个要素：状态state、动作action、转移概率P以及奖励函数reward。

在过去，人工智能通过强化学习达成了许多成就。然而，先前的方法缺乏可泛化性并且只能在定义在相当低维空间的问题有效。随着深度神经网络的广泛使用，函数逼近和特征学习这两大法宝使我们不断克服这些问题。

深度学习的优势在机器学习的许多领域都有重大作用，显著地提升了在经典任务上的表现，例如目标检测，文本识别和语言翻译\cite{9}。深度学习最重要的特征就是深度神经网络可以自动地从图像、文本和声音等高维数据中抽象出简洁的低维特征。通过在深度神经网络中设计启发式的偏差，尤其是层级表示，机器学习使用者在解决维度灾难方面做出了有效的进步\cite{10}。随着使用深度学习算法的强化学习算法，深度强化学习算法的使用，深度学习也同样地加速了强化学习的进步。

深度学习使强化学习能够泛化应用到先前极为棘手的决策问题，比如高维状态-动作空间。在最近的强化学习工作中有三项工作的成功极为突出。

首先是深度强化学习的革命的临门一脚，能够以人类水平从图像像素级别学习游玩雅达利2600个电子游戏的智能体\cite{11}。通过为强化学习中函数逼近技术的不稳定性提供解决方案，这项工作首次令人信服地证明了强化学习智能体可以仅基于奖励信号在原始的高位观察结果上进行训练。

第二个突出的成功是开发了混合深度强化学习系统AlphaGo\cite{12}，它在击败了围棋领域的人类世界冠军。与主导围棋系统的人工设计的下棋策略不同，AlphaGo是由使用监督学习和强化学习训练的神经网络组成，并结合传统的启发式搜索算法，即蒙特卡洛搜索算法。

最新的令人惊讶的工作是由OpenAI基于Dota 2应用场景开发的通用AI系统Open Five，它通过学习团队合作、长期规划和隐藏信息，开始捕捉到真实世界复杂性和连续性，并在5V5的Dota 2游戏中击败了人类顶尖的职业选手。OpenAI Five表明，当前的深度强化学习可以实现大规模的长期规划。

深度强化学习算法已经应用与广泛的问题，例如机器人技术，其中一些机器人可以直接从现实世界的摄像机输入学习控制策略\cite{13}\cite{14}，而取代了人工设计的控制区或者从机器人状态的低维特征中学习。为了向更强大的智能体迈进。深度强化学习已被用于创建可以进行元学习的智能体\cite{15}\cite{16}，允许模型推广到他们从未见过的复杂视觉环境。

虽然电子游戏是一个有趣的挑战，但是学习如何玩雅达利或者Dota电子游戏并不是深度强化学习的最终目标。深度强化学习背后的驱动力之一是创建能够学习如何适应现实世界的智能系统。从调度管理到装载物品，深度强化学习可以增加能够被自主学习的物理任务的数量。但是深度强化学习不知预测，因为强化学习是通过反复验证来解决优化问题的一般方法。从设计最先进的机器翻译模型到构建新的优化功能，深度强化学习已经被用于处理各种机器学习任务。并且，与深度学习在机器人学习的许多分支中的应用相同，在未来深度强化学习将是构建通用人工智能系统的重要组成部分。

\section{研究内容}
本文将使用轮式机器人中应用最广泛的操作系统ROS作为研究平台。ROS因具有完备的跨平台消息转递机制和进程处理能力而广泛应用于机器人研究中。Gazebo是ROS平台上一种功能强大的仿真环境模拟平台，本文使用Gazebo作为仿真和模拟实验的主要平台。Tensorflow作为应用最广泛的开源深度学习框架之一，能够使开发者迅速构建深度学神经网络模型，研究深度学习算法，本文采用Tensorflow作为搭建深度神经网络的基础框架。OpenAI gym 是一个应用于开发搭建和比较研究强化学习的开源工具组件，它提供了简单易用的强化学习环境搭建框架接口和模板。本文使用OpenAI gym 强化学习环境接口来封装ROS中轮式机器人感知、通信和控制等功能，使环境与全自主轮式机器人决策系统解耦，从而能够更加方便地在仿真环境与真实环境中切换。

本文的主要研究对象是全自主的轮式机器人，通过在轮式机器人上应用深度强化学习模型，期望使其具有在复杂真实环境场景下自主与敌方机器人作战的能力，即能够自主巡航、索敌、追踪和攻击。本文的主要工作有如下几个方面：
\vspace{-10pt}
\begin{enumerate}
	\item 深入分析目前主流的强化学习模型，解析其理论本质。
	\item 搭建仿真平台，模拟在复杂条件下全自主轮式机器人的决策过程。
	\item 探索深度强化学习、逆强化学习和模仿学习等多种模型与方法，以在轮式机器人上实现决策功能。
	\item 在现有理论基础上，针对轮式机器人决策问题，对网络结构、奖励函数和训练方法上做出改进。
\end{enumerate}

\section{章节安排}
本文的章节安排如下:

第\ref{preface}章为绪论，介绍了轮式机器人决策系统的概况，研究意义，给出了本文的设计方法，最后介绍了本文的研究目的，内容及结构。

第\ref{introduction}章概述轮式机器人决策系统概述。定义了本文探究的问题边界，概括介绍了轮式机器人控制系统架构，定义了轮式机器人决策系统和接口。

第\ref{rl}章详细分析了强化学习算法研究与分析，阐述了本文实验中主要使用的三种深度强化学习方法，DQN、DDPG以及多智能体异步DDPG。

第\ref{experiment}章详细分析了实验数据采集与分析，分析了在仿真平台与真实环境下的训练特点与挑战，展示了实验数据与结果。

第\ref{conclusion}章总结全文，并展望未来的研究工作。

